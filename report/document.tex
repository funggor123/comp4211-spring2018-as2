\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}


% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{COMP4901I - Building Interactive Intellengent Agent Assignment 3 Report}

\author{%
	Cheng Chi Fung \\
	\texttt{cfchengac@connect.ust.hk} \\
}

\begin{document}

\maketitle

\section{Data}

\subsection{Data Cleaning}
awdawdawd

\subsection{Data Statistics}
awdawdawd

\section{Implement ConvNet with PyTorch}

\subsection{Embedding Results}
awdawdawd

\subsection{Hyperparameters Tuning Results}
awdawdawd

\section{Results and Analysis}
awdawdawd

\subsection{Development Set Accuracy and Test set Accuracy}

\subsection{Analysis}

\section{Bonus}

\subsection{Dynamic Padding}
The following are the screen shot of the Predictor Network, CNN Encoder Network and loading the pretrained encoder.

\subsection{Pretrained Word Embedding}
For Pretrained Word Embedding, we have tried to replace the original word embedding layer by the pretrained \textbf{word2Vector} with \textbf{Google News corpus} (3 billion running words) word vector model. (Google News Corpus: https://github.com/mmihaltz/word2vec-GoogleNews-vectors). And since the dimension of the embedding matrix is enormously big which cause some memory error while training, we have limited to only use ten thousands of vocabs. All the above process can be easily done through by a python libarary named \textbf{gensim}.

And the following are the results of using pretrained embedding.

\subsection{Other CNN Architectures}
For other CNN archiectures, we have implmented character CNN by following the paper \textbf{Character-level Convolutional Networks for Text Classification}. (https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf) 

Same as the paper, we have defined a list of characters which includes 26 English letters, 10 digits, 34 special characters and one blank characters. (\textbf{70 Characters in total})

In the later part, we transfer those characters as 1-hot encoding and used it to create the sentence vectors for each sentences. For unknown characters, blank characters are used to replace it. The sentence vectors would then be inputed into the CNN with the following archiecture which is quite similiar to the paper.


\begin{table}[htb]
\caption{Char CNN Archiecture we used}
	\label{sample-table}
	\centering
\begin{tabular}{lllll}
\toprule
		\cmidrule{1-5}
		Layer & Layer types & Kernel Size & Pooling Size / is Dropout & Number of Filters 		\\
		\midrule
 			1 & Embedding & 100 & -- & -- \\
 			2 & Conv2d & 7 & 3 & 256 \\
 			3 & Conv1d & 7 & 3 & 256 \\
 			4 & Conv1d & 3 & -- & 256 \\
 			5 & Conv1d & 3 & -- & 256\\
 			6 & Conv1d & 3 & -- & 256 \\
 			7 & Conv1d & 3 & 3 & 256 \\
 			8 & Linear & 1024 & Yes & -- \\
 			9 & Linear & 1024 & Yes & -- \\
 			10 & Linear & 3 & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

And the following are the results of using Char CNN. 












\end{document}

